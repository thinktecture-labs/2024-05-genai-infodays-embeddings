{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare embeddings performance\n",
    "\n",
    "We use different approaches to create embeddings from the same texts and compare their performance.\n",
    "\n",
    "## Configuration:\n",
    "\n",
    "Please select the model you want to use for the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "llm_source = \"openai\" # openai or hf for huggingface\n",
    "embedding_source = \"openai\" # openai or hf for huggingface\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "#llm_model = \"gpt-4-1106-preview\"\n",
    "temperature = 0\n",
    "\n",
    "embeddings_model = \"text-embedding-ada-002\"\n",
    "\n",
    "markdown_documents_path = \"C:\\\\Dev\\\\tt\\\\tt-readme\"\n",
    "\n",
    "use_cached_documents = True\n",
    "use_cached_transforms = True\n",
    "reindex_documents = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different approaches of indexing\n",
    "\n",
    "This will\n",
    "- create a question for each document,\n",
    "- create an answer for each document and\n",
    "- summarize each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split markdown contents of the TT Readme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping loading documents from markdown files\n"
     ]
    }
   ],
   "source": [
    "if use_cached_documents:\n",
    "    print(\"Skipping loading documents from markdown files\")\n",
    "else:\n",
    "\n",
    "    from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "    from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "    readme_documents = DirectoryLoader(\n",
    "        markdown_documents_path,\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=TextLoader\n",
    "        ).load()\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "    ]\n",
    "\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "\n",
    "    split_documents = []\n",
    "    for doc in readme_documents:\n",
    "        result = splitter.split_text(doc.page_content)\n",
    "\n",
    "        if isinstance(result, list):\n",
    "            for res in result:\n",
    "                res.metadata.update(doc.metadata)\n",
    "            split_documents.extend(result)\n",
    "        else:\n",
    "            result.metadata.update(doc.metadata)\n",
    "            split_documents.append(result)\n",
    "\n",
    "    # For brevity, reduce amount of entries to a few only\n",
    "    # split_documents = split_documents[50:60]\n",
    "\n",
    "    index  = 1\n",
    "    for doc in split_documents:\n",
    "        doc.metadata[\"index\"] = index\n",
    "        index += 1\n",
    "        doc.metadata[\"original_content\"] = doc.page_content\n",
    "        #print(doc.metadata)\n",
    "        #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the data to files or load cached files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from file\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if (use_cached_documents):\n",
    "    print(\"Loading documents from file\")\n",
    "    with open(\"cache/split_documents.pickle\", \"rb\") as f:\n",
    "        split_documents = pickle.load(f)\n",
    "else:\n",
    "    print(\"Writing documents to file\")\n",
    "    with open(\"cache/split_documents.pickle\", \"wb\") as f:\n",
    "        pickle.dump(split_documents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Massage content into new embedding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=llm_model, temperature=temperature)\n",
    "\n",
    "def build_chain(prompt):\n",
    "    return LLMChain(llm=llm, prompt=PromptTemplate(input_variables=[\"input\"], template=prompt))\n",
    "\n",
    "question_chain = build_chain(\"Formuliere drei verschiedene deutsche Fragen, die der folgende Text beantwortet: {input}\")\n",
    "answer_chain = build_chain(\"Erkläre in zwei bis drei deutschen Sätzen, was der folgende Text beantwortet: {input}\")\n",
    "summarize_chain = build_chain(\"Erstelle eine kurze deutsche Zusammenfassung des folgenden Textes: {input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached file questions\n",
      "Loading cached file answers\n",
      "Loading cached file summaries\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def transform_documents(chain, file):\n",
    "    if use_cached_transforms:\n",
    "        print(f\"Loading cached file {file}\")\n",
    "        with open(f\"cache/{llm_model}_{file}_documents.pickle\", \"rb\") as f:\n",
    "            result = pickle.load(f)\n",
    "        return result\n",
    "    else:\n",
    "        result = copy.deepcopy(split_documents)\n",
    "        for doc in result:\n",
    "            print(f\"Transforming {file} document {doc.metadata['index']} with model {llm_model}\")\n",
    "            doc.metadata[\"original_content\"] = copy.copy(doc.page_content)\n",
    "            doc.page_content = chain.run(doc.page_content)\n",
    "        print(f\"Writing {file} documents from model {llm_model} to file\")\n",
    "        with open(f\"cache/{llm_model}_{file}_documents.pickle\", \"wb\") as f:\n",
    "            pickle.dump(result, f)\n",
    "        return result\n",
    "\n",
    "question_documents = transform_documents(question_chain, \"questions\")\n",
    "answer_documents = transform_documents(answer_chain, \"answers\")\n",
    "summary_documents = transform_documents(summarize_chain, \"summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = None\n",
    "\n",
    "if embedding_source == \"openai\":\n",
    "    embeddings = OpenAIEmbeddings(model=embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "def store(documents, collection_name):\n",
    "    Qdrant.from_documents(\n",
    "        documents,\n",
    "        url=\"http://localhost:6333\",\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        force_recreate=True,\n",
    "    )\n",
    "\n",
    "pure_collection = f\"{embeddings_model}-{llm_model}-p\"\n",
    "question_collection = f\"{embeddings_model}-{llm_model}-q\"\n",
    "answer_collection = f\"{embeddings_model}-{llm_model}-a\"\n",
    "summary_collection = f\"{embeddings_model}-{llm_model}-s\"\n",
    "\n",
    "collections = [pure_collection, question_collection, answer_collection, summary_collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings and store them in different collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "if reindex_documents:\n",
    "    store(split_documents, pure_collection)\n",
    "    store(question_documents, question_collection)\n",
    "    store(answer_documents, answer_collection)\n",
    "    store(summary_documents, summary_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search with a query in the different indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Was mache ich, wenn ich meinen letzten Zug verpasst habe?\",\n",
    "    \"Nach wie vielen Jahren kann ich mein Notebook erneuern?\",\n",
    "    \"Was ist MITOD?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching text-embedding-ada-002-gpt-3.5-turbo-p for Was mache ich, wenn ich meinen letzten Zug verpasst habe?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-p for Nach wie vielen Jahren kann ich mein Notebook erneuern?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-p for Was ist MITOD?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-q for Was mache ich, wenn ich meinen letzten Zug verpasst habe?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-q for Nach wie vielen Jahren kann ich mein Notebook erneuern?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-q for Was ist MITOD?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-a for Was mache ich, wenn ich meinen letzten Zug verpasst habe?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-a for Nach wie vielen Jahren kann ich mein Notebook erneuern?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-a for Was ist MITOD?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-s for Was mache ich, wenn ich meinen letzten Zug verpasst habe?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-s for Nach wie vielen Jahren kann ich mein Notebook erneuern?\n",
      "Searching text-embedding-ada-002-gpt-3.5-turbo-s for Was ist MITOD?\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "def search(collection, query):\n",
    "    return Qdrant(client, collection, embeddings)._similarity_search_with_relevance_scores(query)\n",
    "\n",
    "collections = [pure_collection, question_collection, answer_collection, summary_collection]\n",
    "\n",
    "result_table = []\n",
    "result_table.append([\"Collection\"] + queries)\n",
    "\n",
    "for collection in collections:\n",
    "    row = []\n",
    "    for query in queries:\n",
    "        print(f\"Searching {collection} for {query}\")\n",
    "        search_results = search(collection, query)\n",
    "\n",
    "        row.append(\"\\n\".join([f\"{document.metadata['index']} - {score}\" for document, score in search_results]))\n",
    "\n",
    "    result_table.append([collection] + row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-------------------------------------------------------------+-----------------------------------------------------------+------------------+\n",
      "| Collection                             | Was mache ich, wenn ich meinen letzten Zug verpasst habe?   | Nach wie vielen Jahren kann ich mein Notebook erneuern?   | Was ist MITOD?   |\n",
      "+========================================+=============================================================+===========================================================+==================+\n",
      "| text-embedding-ada-002-gpt-3.5-turbo-p | 46 - 0.81136477                                             | 47 - 0.77842814                                           | 95 - 0.8336302   |\n",
      "|                                        | 44 - 0.8041333                                              | 81 - 0.7779989                                            | 136 - 0.77080166 |\n",
      "|                                        | 45 - 0.79958373                                             | 103 - 0.775543                                            | 14 - 0.76829547  |\n",
      "|                                        | 29 - 0.7978194                                              | 30 - 0.7751603                                            | 89 - 0.7670145   |\n",
      "+----------------------------------------+-------------------------------------------------------------+-----------------------------------------------------------+------------------+\n",
      "| text-embedding-ada-002-gpt-3.5-turbo-q | 44 - 0.85856265                                             | 31 - 0.7993312                                            | 95 - 0.8458184   |\n",
      "|                                        | 46 - 0.82024884                                             | 105 - 0.78937376                                          | 94 - 0.7849594   |\n",
      "|                                        | 71 - 0.7941201                                              | 103 - 0.7832554                                           | 14 - 0.7646515   |\n",
      "|                                        | 40 - 0.7928817                                              | 127 - 0.7822725                                           | 27 - 0.75978094  |\n",
      "+----------------------------------------+-------------------------------------------------------------+-----------------------------------------------------------+------------------+\n",
      "| text-embedding-ada-002-gpt-3.5-turbo-a | 44 - 0.8587989                                              | 47 - 0.7937851                                            | 95 - 0.8630152   |\n",
      "|                                        | 46 - 0.8211136                                              | 101 - 0.78723824                                          | 94 - 0.77524334  |\n",
      "|                                        | 45 - 0.8030342                                              | 30 - 0.7823651                                            | 17 - 0.76892066  |\n",
      "|                                        | 29 - 0.8002844                                              | 64 - 0.7798604                                            | 14 - 0.7678326   |\n",
      "+----------------------------------------+-------------------------------------------------------------+-----------------------------------------------------------+------------------+\n",
      "| text-embedding-ada-002-gpt-3.5-turbo-s | 44 - 0.8288317                                              | 31 - 0.7919528                                            | 95 - 0.8599425   |\n",
      "|                                        | 46 - 0.81279695                                             | 81 - 0.7916113                                            | 136 - 0.7734033  |\n",
      "|                                        | 29 - 0.8032762                                              | 47 - 0.7893325                                            | 53 - 0.76869816  |\n",
      "|                                        | 8 - 0.8008756                                               | 103 - 0.78563046                                          | 31 - 0.76784086  |\n",
      "+----------------------------------------+-------------------------------------------------------------+-----------------------------------------------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(result_table, tablefmt=\"grid\", headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To check a result, put the index in the following cell and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es ist kurz vor Mitternacht, dein Flug hat sich verspätet und der letzte Zug nach Hause/zum Kunde ist schon weg? No worries! Suche dir ein Hotel in der Nähe oder checke ein Taxi (frage aber günstigerweise vor Fahrtbeginn noch einem Fixpreis, das wirkt Wunder!). Bis 300-400 EUR ist's im Einzelfall kein Problem … damit sollte man von Frankfurt Flughafen bis fast nach Frankreich kommen ;). Falls es dramatisch mehr ist, ist eventuell doch eine Hotelübernachtung wirklich sinnvoller …\n",
      "\n",
      "\n",
      "{'Header 1': 'Auf Reise', 'Header 2': 'Gestrandet im Nirgendwo', 'source': 'C:\\\\Dev\\\\tt\\\\tt-readme\\\\best-practices\\\\arbeiten-mit-kunden\\\\auf-reise.md', 'index': 44, 'original_content': \"Es ist kurz vor Mitternacht, dein Flug hat sich verspätet und der letzte Zug nach Hause/zum Kunde ist schon weg? No worries! Suche dir ein Hotel in der Nähe oder checke ein Taxi (frage aber günstigerweise vor Fahrtbeginn noch einem Fixpreis, das wirkt Wunder!). Bis 300-400 EUR ist's im Einzelfall kein Problem … damit sollte man von Frankfurt Flughafen bis fast nach Frankreich kommen ;). Falls es dramatisch mehr ist, ist eventuell doch eine Hotelübernachtung wirklich sinnvoller …\"}\n",
      "\n",
      "\n",
      "Questions: 1. Was kann ich tun, wenn mein Flug sich verspätet hat und der letzte Zug nach Hause schon weg ist?\n",
      "2. Wie finde ich ein Hotel in der Nähe des Flughafens, wenn mein Flug sich verspätet hat und ich nicht mehr nach Hause komme?\n",
      "3. Wie kann ich den Preis für ein Taxi vor Fahrtbeginn verhandeln, um Kosten zu sparen, wenn ich keinen Zug mehr nach Hause bekomme?\n",
      "\n",
      "\n",
      "Answers: Der Text gibt Ratschläge, was man tun kann, wenn man aufgrund einer Flugverspätung den letzten Zug verpasst hat. Man kann entweder ein Hotel in der Nähe suchen oder ein Taxi nehmen, wobei man vorher den Preis klären sollte. In manchen Fällen ist es auch möglich, bis zu 300-400 EUR für die Taxifahrt zu bezahlen, um nach Hause oder zum Kunden zu kommen. Wenn der Preis jedoch deutlich höher ist, ist es möglicherweise sinnvoller, im Hotel zu übernachten.\n",
      "\n",
      "\n",
      "Summary: Wenn dein Flug Verspätung hat und du den letzten Zug nach Hause oder zum Kunden verpasst hast, musst du dir keine Sorgen machen. Du kannst ein Hotel in der Nähe suchen oder ein Taxi nehmen. Es ist ratsam, vor Fahrtbeginn nach einem Festpreis zu fragen, das kann Wunder bewirken. Bis zu 300-400 Euro ist es in manchen Fällen kein Problem, damit kann man fast bis nach Frankreich vom Flughafen Frankfurt kommen. Wenn es jedoch deutlich teurer ist, ist es vielleicht sinnvoller, im Hotel zu übernachten.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "found_index = 44\n",
    "\n",
    "# find the document with the metadata index of the found_index variable\n",
    "\n",
    "found_document = None\n",
    "for doc in split_documents:\n",
    "    if doc.metadata[\"index\"] == found_index:\n",
    "        found_document = doc\n",
    "        break\n",
    "\n",
    "print(f'{found_document.page_content}\\n\\n')\n",
    "print(f'{found_document.metadata}\\n\\n')\n",
    "\n",
    "for doc in question_documents:\n",
    "    if doc.metadata[\"index\"] == found_index:\n",
    "        found_document = doc\n",
    "        break\n",
    "\n",
    "print(f\"Questions: {found_document.page_content}\\n\\n\")\n",
    "\n",
    "for doc in answer_documents:\n",
    "    if doc.metadata[\"index\"] == found_index:\n",
    "        found_document = doc\n",
    "        break\n",
    "\n",
    "print(f\"Answers: {found_document.page_content}\\n\\n\")\n",
    "\n",
    "for doc in summary_documents:\n",
    "    if doc.metadata[\"index\"] == found_index:\n",
    "        found_document = doc\n",
    "        break\n",
    "\n",
    "print(f\"Summary: {found_document.page_content}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Teaser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Document\n",
    "found_index = 31\n",
    "found_document = None\n",
    "for doc in split_documents:\n",
    "    if doc.metadata[\"index\"] == found_index:\n",
    "        found_document = doc\n",
    "        break\n",
    "\n",
    "query = \"Nach wie vielen Jahren kann ich mein Notebook erneuern?\"\n",
    "\n",
    "# Prompt\n",
    "template = f\"\"\"Beantworte die Frage nur aufgrund der folgenenden Informationen:\n",
    "{found_document.page_content}\n",
    "\n",
    "Frage: {query}\n",
    "\"\"\"\n",
    "\n",
    "# RAG chain\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(template)\n",
    "    | ChatOpenAI(model_name = 'gpt-4-1106-preview')\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
